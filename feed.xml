<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>James Gregory</title>
    <description>Jack of all trades, master of none. Principal Consultant at Thoughtworks. Pronouns: he/him
</description>
    <link>https://www.jagregory.com//</link>
    <atom:link href="https://www.jagregory.com//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 23 May 2023 18:21:39 +1000</pubDate>
    <lastBuildDate>Tue, 23 May 2023 18:21:39 +1000</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>How DynamoDB queries behave compared to relational databases</title>
        <description>&lt;p&gt;I wanted to write about how using Async Iterators in Node.JS can make querying DynamoDB tables more pleasant, but I got
side-tracked with explaining why pagination and is more often necessary when using DynamoDB than when using relational
databases.&lt;/p&gt;

&lt;p&gt;So instead of the thing I meant to write about, here’s a refresher on how bounded and unbounded queries are treated
differently by typical relational databases and DynamoDB.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;relational-databases&quot;&gt;Relational databases&lt;/h2&gt;

&lt;p&gt;In relational databases, there’s two kinds of queries to understand: unbounded and bounded.&lt;/p&gt;

&lt;p&gt;An unbounded query is one without an explicit upper limit on the number of records. &lt;em&gt;Unbounded queries are generally a
bad idea&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;With unbounded queries, as there’s always a hard limit somewhere, you’re putting the responsibility for enforcing the
limits of your query onto the underlying database engine or hardware. In most cases, the behaviour you see with
unbounded queries is they will run for as long as possible until they either complete and return all the data you
requested or fail and return an error. For &lt;em&gt;not too large&lt;/em&gt; tables the observable behaviour is a slower query than usual
which finally returns all the data, but for large tables you run the risk of hitting timeouts (in the database client or
server, or lower in the stack) or exhausting resources like available memory which would cause a query to fail and
potentially affect the health of your database. Unbounded queries can be insidious when they return data from small
tables which are slowly accumulating data, until one day they’re no longer small and you start experiencing timeouts or
out of memory issues.&lt;/p&gt;

&lt;p&gt;A bounded query is a query which you’re putting a deliberate and explicit upper limit on the number of records that will
be returned by the query. In a relational database (such as PostgreSQL or SQL Server) you do this using queries
with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;limit&lt;/code&gt; (or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top&lt;/code&gt; in SQL Server), such as: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select * from users limit 30&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;When you do that, the database behaves pretty much how you would expect it to: you ask for up to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; rows and it will
return you up to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; rows. By being explicit in how many records you’re expecting to return, assuming you’ve chosen a
reasonable limit, your query duration and resource consumption should be stable regardless of how many rows are present
in the table, even as the table grows.&lt;/p&gt;

&lt;p&gt;To summarise:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bounded queries - you ask for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; rows, the database gives you (up to) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; rows.&lt;/li&gt;
  &lt;li&gt;Unbounded queries - you ask for everything, the database will give you everything or it will fail. All or nothing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dynamodb&quot;&gt;DynamoDB&lt;/h2&gt;

&lt;p&gt;DynamoDB also supports both bounded and unbounded queries, but for all queries it also applies strict constraints which
make all queries bounded in a much more obvious way than relational databases.&lt;/p&gt;

&lt;p&gt;Bounded queries can can use a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Limit&lt;/code&gt; parameter when running a query, but unlike relational databases DynamoDB may
return fewer than that limit &lt;em&gt;even if there are more records available&lt;/em&gt;. This is a crucial difference from relational
databases, and will affect how you design your application.&lt;/p&gt;

&lt;p&gt;From the DynamoDB documentation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The result set for a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Query&lt;/code&gt; is limited to 1 MB per call.&lt;/p&gt;

  &lt;p&gt;The maximum item size in DynamoDB is 400 KB, which includes both attribute name binary length (UTF-8 length) and
attribute value lengths (again binary length). The attribute name counts towards the size limit.&lt;/p&gt;

  &lt;p&gt;– &lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html#limits-api&quot;&gt;API Specific Limits&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;(the same is true for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Scan&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Whilst there’s no hard upper limit on the number of individual records a query or scan can return, they are constrained
by the total data being returned. When a query exceeds this limit, DynamoDB will respond with all the records it was
able to process prior to hitting the limit, regardless of how many were asked for.&lt;/p&gt;

&lt;p&gt;So the maximum you can return from a single query is 1 MB and the largest record you can have is 400 KB, which means in
absolute worst-case scenario if all your records are the largest they’re allowed to be &lt;strong&gt;your query could return just
two items&lt;/strong&gt;! even if you asked for everything.&lt;/p&gt;

&lt;p&gt;It’s also worth mentioning the impact of filter expressions and projections, or rather their non-impact. You may think
that by adding a filter expression (to exclude certain items from your queries) or projection expressions (to limit the
attributes returned) would allow you to avoid the 1 MB limit but that is not the case:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;DynamoDB calculates the number of read capacity units consumed based on item size, not on the amount of data that is
returned to an application. For this reason, the number of capacity units consumed is the same whether you request all
of the attributes (the default behavior) or just some of them (using a projection expression). The number is also the
same whether or not you use a filter expression.&lt;/p&gt;

  &lt;p&gt;– &lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html&quot;&gt;Query operations in DynamoDB&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Without going into too much detail, what this is saying is DynamoDB reads all your data which matches the partition and
range keys you used with the query, and then just before returning it to you it filters out data any which match your
filter expression or projection expressions. By the time the filtering is applied, DynamoDB would have already hit the
capacity limit. The only benefit filtering and projections have is minimising the amount of data which is transferred
over the wire and serialized/deserialized, it does not affect the amount of data DynamoDB reads.&lt;/p&gt;

&lt;p&gt;In a result set which has more records available but weren’t returned to us because of the above limits, DynamoDB will
also return a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; which is the key of the last record in the result set before the limit was exceeded. If
we pass that key as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ExclusiveStartKey&lt;/code&gt; in a subsequent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Query&lt;/code&gt;, DynamoDB will carry on from where it left off and
return you the next batch of records until it either runs out of records to return or it hits a limit again. Repeat that
process until DynamoDB stops returning a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; in the response and you’ve now retrieved all the data for
your query (or scan).&lt;/p&gt;

&lt;h3 id=&quot;what-does-this-look-like-in-an-application&quot;&gt;What does this look like in an application?&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’ve exaggerated the item sizes in these examples to keep them brief. In reality, your items should be considerably
smaller than what’s in these examples and you’ll be getting many more items per-page of results.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, we’re going to query for all the Orders that a particular User has made. We make an initial &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Query&lt;/code&gt; with that
user’s ID as the partition key, and don’t supply a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; because we’re reading the first page of results.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, LastEvaluatedKey: null) -&amp;gt;

  items:
  - order#1  - 100kb
  - order#2  - 100kb
  - order#3  - 100kb
  - order#4  - 100kb
  - order#5  - 100kb
  - order#6  - 100kb
  - order#7  - 100kb
  - order#8  - 100kb
  - order#9  - 100kb
  - order#10 - 100kb
  lastEvaluatedKey: order#10

  1 MB limit reached.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;DynamoDB returned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10&lt;/code&gt; order records for that query, it also returned a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; indicating there is more data
available but it didn’t return it because it hit a limit.&lt;/p&gt;

&lt;p&gt;So we issue a followup query using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ExclusiveStartKey&lt;/code&gt;, &lt;strong&gt;exclusive&lt;/strong&gt; being a significant
word here meaning that it will start returning records &lt;em&gt;after&lt;/em&gt; this start key:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, ExclusiveStartKey: &quot;order#10&quot;) -&amp;gt;

  items:
  - order#11 - 100kb
  - order#12 - 400kb
  - order#13 - 200kb
  - order#14 - 200kb
  - order#15 - 100kb
  lastEvaluatedKey: order#15

  1 MB limit reached.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This time the query returned only returned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;5&lt;/code&gt; order records, much fewer than last time. We can assume from the size of
the individual items that so few items were returned because of those large &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;200kb&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;400kb&lt;/code&gt; items. This is a pretty
consistent behaviour you’ll see when querying DynamoDB tables: the number of items in query result sets varies based on
the size of the data being returned.&lt;/p&gt;

&lt;p&gt;The query also returned a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; again, so we issue another query:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, ExclusiveStartKey: &quot;order#15&quot;) -&amp;gt;

  items:
  - order#16 - 100kb
  - order#17 - 100kb
  - order#18 - 100kb
  lastEvaluatedKey: null

  No more records.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This time DynamoDB returned us &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3&lt;/code&gt; order records and no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt;, indicating it has reached the end of the
records which match our query.&lt;/p&gt;

&lt;p&gt;In our application we should be collecting the records as we’re iterating over the pages so when we finally reach the
last page we can return them as one list to our caller who should be non-the-wiser about this happening (apart from the
duration).&lt;/p&gt;

&lt;h4 id=&quot;a-note-about-limits&quot;&gt;A note about limits&lt;/h4&gt;

&lt;p&gt;If you want to constrain the total number of records that DynamoDB will return, you might think supplying a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Limit&lt;/code&gt; to
the query would do the trick, but just watch out because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Limit&lt;/code&gt; is better thought of as a batch size or page size limit
than an overall total records limit.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is also true in relational databases, but because we can do much larger result sets in relational databases it’s
easy to treat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;limit&lt;/code&gt; as an overall record limit. When you combine a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;limit&lt;/code&gt; with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offset&lt;/code&gt; in PostgreSQL you’ll get
the same behaviour as below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Using the examples above, if we were to ask DynamoDB to return only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;5&lt;/code&gt; records, it would work exactly as you’d expect:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, Limit: 5, LastEvaluatedKey: null) -&amp;gt;

  items:
  - order#1  - 100kb
  - order#2  - 100kb
  - order#3  - 100kb
  - order#4  - 100kb
  - order#5  - 100kb
  lastEvaluatedKey: null

  No more records.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;DynamoDB returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;5&lt;/code&gt; records, and doesn’t have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; because there’s no more data to return.&lt;/p&gt;

&lt;p&gt;However, what if we wanted to return &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; items?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, Limit: 30, LastEvaluatedKey: null) -&amp;gt;

  items:
  - order#1  - 100kb
  - order#2  - 100kb
  - order#3  - 100kb
  - order#4  - 100kb
  - order#5  - 100kb
  - order#6  - 100kb
  - order#7  - 100kb
  - order#8  - 100kb
  - order#9  - 100kb
  - order#10 - 100kb
  lastEvaluatedKey: order#10

  1 MB limit reached.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As before, DynamoDB makes an attempt to return &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; records but stops at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10&lt;/code&gt; because of the aforementioned limits. So
again as before we issue a followup query, but what should we pass as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Limit&lt;/code&gt; this time?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, Limit: ???, ExclusiveStartKey: &quot;order#10&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is where you see the difference with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Limit&lt;/code&gt; being a batch size limit, not a total record limit. You can’t just
pass &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; to every subsequent query because DynamoDB will just keep attempting to fetch &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt; more records every
query.&lt;/p&gt;

&lt;p&gt;Instead, you need to keep a rolling tally yourself, and use that to determine an appropriate limit and stop querying
either when there’s no more &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; or when you hit your total records:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Query(pk: &quot;user1&quot;, Limit: 30 - totalFetchedSoFar, ExclusiveStartKey: &quot;order#10&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;a-note-about-filters&quot;&gt;A note about filters&lt;/h3&gt;

&lt;p&gt;Filters add an extra surprise to the mix: Sometimes a DynamoDB query with a filter applied will return an empty result
set with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; (indicating there’s more data to fetch). This can be surprising the first time it happens.&lt;/p&gt;

&lt;p&gt;If you remember back to the note about filters:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;DynamoDB calculates the number of read capacity units consumed based on item size, not on the amount of data that is
returned to an application. For this reason, the number of capacity units consumed is the same whether you request all
of the attributes (the default behavior) or just some of them (using a projection expression). The number is also the
same whether or not you use a filter expression.&lt;/p&gt;

  &lt;p&gt;– &lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html&quot;&gt;Query operations in DynamoDB&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;DynamoDB is first fetching a page worth of data from your partition and then afterward applying any filters you’ve
provided. What this means is DynamoDB will fetch up to 1 MB of records from your table, and then apply your filter
&lt;em&gt;which may exclude all the records that were just fetched&lt;/em&gt; and so return you an empty result set.&lt;/p&gt;

&lt;p&gt;The main takeaway from this is: rely on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LastEvaluatedKey&lt;/code&gt; to tell if there’s no more data available not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Items.length&lt;/code&gt;
otherwise you may incorrectly assume you’ve fetched all the data when you haven’t.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Queries with relational databases tend to behave predictably. You ask them for an amount of rows and they’ll give you
those rows (or they’ll die trying). They may run out of memory, run out of time, or your users will just move on before
they’ve completed, but they will give you those rows you asked for.&lt;/p&gt;

&lt;p&gt;DynamoDB on the other hand is much less accommodating. Whether you ask DynamoDB for everything or just ask for just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;30&lt;/code&gt;
records, there’s a good chance DynamoDB will decide to give you fewer records than you wanted. You need to factor this
into how you write your data access code, because it’s much more likely to happen than you expect, and this is why
you should almost always implement paginated queries with DynamoDB.&lt;/p&gt;
</description>
        <pubDate>Tue, 23 May 2023 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/dynamodb-queries-vs-relational</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/dynamodb-queries-vs-relational</guid>
        
        
      </item>
    
      <item>
        <title>Improving my dev loop with visual regression testing</title>
        <description>&lt;p&gt;When the pandemic hit we (at my social enterprise side-hustle) quickly built a
video calling web app for medical students to able to continue their
“face-to-face” practice exercises, which later grew to allow actual exam
scenarios for those students, and also then started supporting rehabilitation
for people with traumatic brain injuries.&lt;/p&gt;

&lt;p&gt;It’s been quite a journey and I’ve learned a lot along the way. Recently, I’ve
been trying to improve the dev loop for making visual changes to a delicate part
of the system.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;The video calling feature was built &lt;em&gt;quickly&lt;/em&gt;, adding to an existing ed tech
platform we already had. During the early pandemic, when work was getting a bit
thin for my employer, I took a some time off and built the video calling in a
few weeks. It was a rush to say the least.&lt;/p&gt;

&lt;p&gt;The underlying core services are solid (tying together Twilio APIs with AWS
Lambda), but the UI has been delicate ever since I built it. A bad combination
of a rush on my part, relatively nascent browser tech, and cross-browser/device
compatibility issues.&lt;/p&gt;

&lt;p&gt;Recently, I made some small changes that ended up breaking a small part of the
call experience for a subset of users.&lt;/p&gt;

&lt;p&gt;Fortunately, the issue was minor (not being able to see your own video in the
corner of the screen) but it finally kicked me into try to do something to
improve the reliability of this area which hasn’t really benefitted from the
traditional test pyramid.&lt;/p&gt;

&lt;h2 id=&quot;step-1---refactor-the-calling-ui-to-work-isolated-in-storybook&quot;&gt;Step 1 - Refactor the calling UI to work isolated in Storybook&lt;/h2&gt;

&lt;p&gt;This was a couple of big changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pulling queries up the stack to make it easier to mock behaviour in stories,
all data fetching ended up outside the core calling UI.&lt;/li&gt;
  &lt;li&gt;make it possible to fake cameras, microphones, and screen sharing states. The
combination of these two things meant I could create Storybook stories for
scenarios like “only one person has joined the call”, “two people in the
call, one person has their camera off”, “one person screen sharing, another
person with a portrait camera” etc…&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With the Storybook stories I could now eyeball these common variations easily
enough, which gave me a much quicker feedback loop than actually initiating a
video call.&lt;/p&gt;

&lt;h2 id=&quot;step-2---visual-regression-tests-against-the-storybook-stories&quot;&gt;Step 2 - Visual regression tests against the Storybook stories&lt;/h2&gt;

&lt;p&gt;Using &lt;a href=&quot;https://percy.io/&quot;&gt;percy.io&lt;/a&gt;, I now take a screenshot of each of those
stories, in several browsers, at several resolutions and orientations, and then
diff them against previous snapshots.&lt;/p&gt;

&lt;p&gt;I haven’t hooked this into CI (and I’m undecided if I will) but it’s been very
useful as a form of regression testing for when I’m iterating over some purely
visual (e.g. CSS) changes.&lt;/p&gt;

&lt;p&gt;I now make the change in Chrome and test it manually in a few stories, but then
kick off the percy.io tests and see what it tells me. Much easier than manually
comparing across different browsers and devices.&lt;/p&gt;

&lt;p&gt;It’s still not perfect, but it’s a big improvement.&lt;/p&gt;

&lt;p&gt;Now I just need to finish the simple task of making my calling UI a pleasant
experience for my users. 😅&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cross-posted from Mastodon: https://aus.social/@jagregory/109366550345741717&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 18 Nov 2022 00:00:00 +1100</pubDate>
        <link>https://www.jagregory.com//writings/more-reliable-visual-changes</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/more-reliable-visual-changes</guid>
        
        
      </item>
    
      <item>
        <title>Multi-region CloudTrail with logs in another AWS Account</title>
        <description>&lt;p&gt;I had one &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;CloudTrail&lt;/a&gt;. Then I had many CloudTrails. Now I have
only one once again, but a better configured one.&lt;/p&gt;

&lt;p&gt;In this post I walk through my experience of migrating from a less-than-ideal hand-rolled
multi-region CloudTrail setup to an single multi-region CloudTrail with a sprinkling of extra
security by using a separate AWS account to store the logs.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;what-is-cloudtrail&quot;&gt;What is CloudTrail?&lt;/h2&gt;

&lt;p&gt;Before we get started, what actually is CloudTrail and why would I want to use it?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt; The tag line on the &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;CloudTrail landing page&lt;/a&gt; is
the official word, but informally CloudTrail collects logs of all the management activity on your
AWS account and dumps it into an S3 bucket for you to look at later (hopefully never).&lt;/p&gt;

&lt;p&gt;Below is a truncated example of a single CloudTrail event, from CloudTrail itself querying my
bucket.&lt;/p&gt;

&lt;p&gt;You can see the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eventType&lt;/code&gt;, an AWS API call, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eventName&lt;/code&gt; with the actual call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetBucketAcl&lt;/code&gt;,
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;userIdentity&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cloudtrail.amazonaws.com&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resources&lt;/code&gt; it accessed.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1.08&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userIdentity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;AWSService&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;invokedBy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cloudtrail.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventTime&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2021-10-26T00:49:57Z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventSource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;s3.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;GetBucketAcl&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;awsRegion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ap-southeast-2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;AwsApiCall&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventCategory&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Management&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;resources&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;accountId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;AWS::S3::Bucket&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ARN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:s3:::mybucketname&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Why would you use it?&lt;/strong&gt; Auditing activity on your AWS account. If ever you have a security
incident or are just curious “who deleted that S3 bucket?” then CloudTrail is what you need enabled
to find that out. Without CloudTrail enabled it’s going to be considerably more difficult for you to
investigate issues. As a bonus, CloudTrail is free for management logging, you just pay for the
storage of events in S3.&lt;/p&gt;

&lt;p&gt;When asked what the one thing everyone should do right now to improve their security posture, &lt;a href=&quot;https://aws.amazon.com/blogs/security/definitely-not-an-aws-security-profile-corey-quinn-a-cloud-economist-who-doesnt-work-here/&quot;&gt;Corey
Quinn
said&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Easy. Log into the console of your organization’s master account and enable &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;AWS CloudTrail&lt;/a&gt;
for all regions and all accounts in your organization. Direct that trail to a locked-down S3
bucket in a completely separate, highly restricted account, and you’ve got a forensic log of all
management options across your estate.&lt;/p&gt;

  &lt;p&gt;Worst case, you’ll thank me later. Best case, you’ll never need it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-state-of-my-cloudtrail&quot;&gt;The state of my CloudTrail&lt;/h2&gt;

&lt;p&gt;I did have CloudTrail enabled, so not a bad starting point; however, the setup did leave a few
things to desire, especially as it’d grown over the past few years.&lt;/p&gt;

&lt;p&gt;In the beginning I had one CloudTrail for the one region in use. The trail logged into an S3 bucket
in the same AWS account.&lt;/p&gt;

&lt;p&gt;As my AWS footprint expanded to another couple more regions, I replicated that same setup (instead
of switching to a &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/&quot;&gt;multi-region
trail&lt;/a&gt;,
which I was oblivious to) to each additional region. Later, I pushed it to all regions for peace of
mind. That left me with twenty trails and twenty S3 buckets, all in the same AWS account. Definitely
not ideal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/too-many-cloudtrails.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What I wanted to do was to consolidate everything down to a single multi-region CloudTrail and a
single S3 bucket. For added security, I also wanted to put the S3 bucket in a separate AWS account.
In my original setup, anyone who compromises credentials on the main account may also be able to
remove any record of their activity if they could gain access to the CloudTrail S3 bucket. In the
new setup, the severely locked down secondary AWS account would at least make it much more difficult
for someone to hide their tracks.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-a-multi-region-cloudtrail-with-an-s3-bucket-in-another-account&quot;&gt;Setting up a multi-region CloudTrail with an S3 bucket in another account&lt;/h2&gt;

&lt;p&gt;To have your S3 bucket in another AWS account you’re going to need another AWS account! I suggest
you make a completely new account for this, the only thing this account will have in is an S3 bucket
for your CloudTrail logs, by using a new account you reduce the risk of someone else wandering in
for other reasons.&lt;/p&gt;

&lt;p&gt;Once you’ve made the account, follow the usual security practices and &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials&quot;&gt;lock down the root
account&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If the account is within an Organisation, pay close attention to the cross-account role if there is
one. This role, usually called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OrganizationAccountAccessRole&lt;/code&gt;, allows admin access to the new
account from your main AWS account. Make sure nobody can assume that role who shouldn’t be able to,
otherwise they’ll be able to jump into the new account and tamper with your S3 bucket (defeating the
point of having a separate account).&lt;/p&gt;

&lt;p&gt;Now in your newly locked down account, create an S3 bucket for your CloudTrail logs. Lock that thing
down too, no public access, nobody has write access. You’ll need to &lt;a href=&quot;https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-set-bucket-policy-for-multiple-accounts.html&quot;&gt;setup a policy to your bucket
to allow CloudTrail write
access&lt;/a&gt;
across accounts.&lt;/p&gt;

&lt;p&gt;Whilst you’re at it, you might want to setup a &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html&quot;&gt;lifecycle
policy&lt;/a&gt; on your
new bucket to transition older logs to cold storage to save you a little bit of money.&lt;/p&gt;

&lt;p&gt;Now back over in your main AWS account create (or update) your CloudTrail and point it at the new S3
bucket in the other account, turn &lt;a href=&quot;https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html&quot;&gt;on the multi-region
feature&lt;/a&gt;
on the trail (I think all new trails made in the console are multi-region by the way), and then save
it and you’re done.&lt;/p&gt;

&lt;p&gt;At this point you have a CloudTrail which monitors all your regions (and automatically expands to
new regions as they become available) and sends logs to an S3 bucket in a separate locked-down AWS
account.&lt;/p&gt;

&lt;p&gt;If you’re like me though, you also have twenty existing buckets which have been accumulating logs
for years that you’d like to keep…&lt;/p&gt;

&lt;h2 id=&quot;consolidating-up-my-historical-trails&quot;&gt;Consolidating up my historical trails&lt;/h2&gt;

&lt;p&gt;As I mentioned at the start, I’d ended up with twenty CloudTrails each with their own bucket. Those
buckets had been accumulating logs for five years and, as I’m obligated to keep logs around for
seven years, I didn’t want to lose these logs as part of my consolidation.&lt;/p&gt;

&lt;p&gt;This is easy, right? “Just move the files from one bucket to the new one!”&lt;/p&gt;

&lt;p&gt;Not so fast. I’ve just locked down the CloudTrail S3 bucket so nobody can write to it apart from
CloudTrail, and also the bucket is now in a different AWS account and cross-account copying is a
little trickier.&lt;/p&gt;

&lt;p&gt;Fortunately, this &lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-objects-account/&quot;&gt;“How can I copy S3 objects from another AWS
account?”&lt;/a&gt; FAQ was
covered most of what I needed.&lt;/p&gt;

&lt;p&gt;Firstly, I needed to punch a hole in my shiny new security policy by granting write access to my AWS
user so I could copy the files. Watch out for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;s3:x-amz-acl&quot;: &quot;bucket-owner-full-control&quot;&lt;/code&gt;
requirement. I revoked this access as soon as the copying was complete.&lt;/p&gt;

&lt;p&gt;Secondly, configure &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html&quot;&gt;Object
Ownership&lt;/a&gt; on the
central S3 bucket so the new AWS account owns the files that are copied to it.&lt;/p&gt;

&lt;p&gt;Then repeatedly for each region, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 sync --storage-class GLACIER --acl
bucket-owner-full-control s3://original-cloudtrail-bucket s3://new-cloudtrail-bucket&lt;/code&gt; to copy the
files over. Thanks to the way CloudTrail logs to the bucket, I didn’t need to do any path
manipulation, it all can copy into the single bucket without any changes. Again, watch out for the
ACL setting for the bucket ownership, and optionally set the storage class for the files whilst
you’re copying.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lots of time passes.&lt;/em&gt; Turns out copying files that have been collected over five years from twenty
different regions takes a lot of time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/cloudtrail-s3-sync-logs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally when that’s all done, delete those old buckets and any other CloudTrails and patch up any
security holes you created. Nice, twenty trails and buckets down to just one.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;One CloudTrail better than none, but also better than many. Multi-region (and multi-account)
CloudTrails are great and pushing your logs off to another AWS account can help reduce your security
risk by reducing the blast radius of compromised credentials.&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Oct 2021 00:00:00 +1100</pubDate>
        <link>https://www.jagregory.com//writings/multi-region-cloudtrail</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/multi-region-cloudtrail</guid>
        
        
      </item>
    
      <item>
        <title>Keeping API keys and environment-specifics out of your OpenTelemetry config</title>
        <description>&lt;p&gt;When I was setting up &lt;a href=&quot;https://www.honeycomb.io/&quot;&gt;Honeycomb&lt;/a&gt; with my Lambda functions there was something that bothered me: the &lt;a href=&quot;https://opentelemetry.io/&quot;&gt;OpenTelemetry&lt;/a&gt; config file contained my API keys and environment-specific details. I needed to keep my bundle environment-agnostic, and I really didn’t want to be committing API keys to version control. You can read more about the adventure itself in &lt;a href=&quot;/writings/getting-honeycomb-working-with-my-aws-lambda-functions&quot;&gt;my other post&lt;/a&gt;, or continue reading this post for my solution.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;OpenTelemetry supports environment variables to override parts of the config file, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OTEL_EXPORTER_OTLP_ENDPOINT&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OTEL_EXPORTER_OTLP_HEADERS&lt;/code&gt;. In theory, you could use these to customise your config file. Unfortunately, the AWS Distro for OpenTelemetry on AWS Lambda doesn’t seem to honour these variables.&lt;/p&gt;

&lt;p&gt;Fortunately, the &lt;a href=&quot;https://opentelemetry.io/docs/collector/configuration&quot;&gt;OpenTelemetry Collector documentation&lt;/a&gt; has a section on &lt;a href=&quot;https://opentelemetry.io/docs/collector/configuration/#configuration-environment-variables&quot;&gt;environment variables&lt;/a&gt; which says “the use and expansion of environment variables is supported in the Collector configuration”.&lt;/p&gt;

&lt;p&gt;If you update your config file to reference other environment variables, you can keep the config file environment-agnostic and remove the need to embed API keys. For example, with Honeycomb you can configure your exporter like so:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;exporters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;otlp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;api.honeycomb.io:443&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-honeycomb-team&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$HONEYCOMB_TEAM_KEY&quot;&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-honeycomb-dataset&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$HONEYCOMB_DATASET$&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’d prefer if I didn’t have to store a sensitive key in an environment variable either, but it’s a step in the right direction and is better than committing it to version control. There’s an &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/issues/2469&quot;&gt;open design document on external secret management in OpenTelemetry&lt;/a&gt; which I’ll be keeping an eye on. I’d be much happier if I could just reference an AWS Secrets Manager secret by name in the config, one can dream. Until then, I think this is the best I can do.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Oct 2021 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/keeping-api-keys-and-environment-specifics-out-of-your-opentelemetry-config</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/keeping-api-keys-and-environment-specifics-out-of-your-opentelemetry-config</guid>
        
        
      </item>
    
      <item>
        <title>Honeycomb and OpenTelemetry with AWS Lambda and Node.js (reference)</title>
        <description>&lt;p&gt;This is a condensed guide to how I configured AWS Lambda to work with
&lt;a href=&quot;https://honeycomb.io&quot;&gt;Honeycomb&lt;/a&gt; and &lt;a href=&quot;https://opentelemetry.io&quot;&gt;OpenTelemetry&lt;/a&gt;.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;I had a few struggles when getting &lt;a href=&quot;https://www.honeycomb.io/&quot;&gt;Honeycomb&lt;/a&gt; and &lt;a href=&quot;https://opentelemetry.io/&quot;&gt;OpenTelemetry&lt;/a&gt; working with my existing Node.js Lambda functions, which rely on automatic and manual instrumentation. You can read more about that &lt;a href=&quot;/writings/getting-honeycomb-working-with-my-aws-lambda-functions&quot;&gt;over here&lt;/a&gt; and the page you’re on right now is the condensed version with my working solution.&lt;/p&gt;

&lt;h2 id=&quot;step-1-get-auto-instrumentation-working-with-your-lambda-function&quot;&gt;Step 1: Get auto-instrumentation working with your Lambda function&lt;/h2&gt;

&lt;p&gt;The first part is getting auto-instrumentation working with OpenTelemetry. This is pretty easy, although I did hit a couple of issues related to bundling.&lt;/p&gt;

&lt;p&gt;Use the &lt;a href=&quot;https://aws-otel.github.io/docs/getting-started/lambda/lambda-js&quot;&gt;Lambda Layer&lt;/a&gt; provided by &lt;a href=&quot;https://aws-otel.github.io/docs/getting-started/lambda&quot;&gt;AWS Distro for OpenTelemetry Lambda&lt;/a&gt;. Add the layer to your Lambda function and set an environment variable to enable OpenTelemetry. Invoke your function and you should see in your CloudWatch logs a heap of new logs from the OpenTelemetry Collector.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Or if you’re like me you’ll hit a &lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/issues/99&quot;&gt;known issue&lt;/a&gt; and have to tweak your bundling config, and &lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/issues/99&quot;&gt;another one&lt;/a&gt; about CDK and esbuild.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point you have made no modifications to your application code and auto-instrumentation should be working. The traces will be sent to X-Ray by default, so next is to configure OpenTelemetry to send data to Honeycomb.&lt;/p&gt;

&lt;h2 id=&quot;step-2-customise-your-configuration-to-point-to-honeycomb&quot;&gt;Step 2: Customise your configuration to point to Honeycomb&lt;/h2&gt;

&lt;p&gt;To customise AWS Distro for OpenTelemetry you have to add your own config file to your Lambda functions and set an environment variable to tell OpenTelemetry to use your overriden config file. Read &lt;a href=&quot;https://aws-otel.github.io/docs/getting-started/lambda#custom-configuration-for-the-adot-collector-on-lambda&quot;&gt;custom configuration for the ADOT Collector on Lambda&lt;/a&gt; for more information. There’s also &lt;a href=&quot;https://aws-otel.github.io/docs/components/otlp-exporter#honeycomb&quot;&gt;instructions for Honeycomb&lt;/a&gt; on the AWS Distro site.&lt;/p&gt;

&lt;p&gt;If you redeploy your Lambda function now you should start seeing traces appear in Honeycomb.&lt;/p&gt;

&lt;p&gt;At this point, I wasn’t very happy with having API Keys and environment-specific information in the OpenTelemetry config file. If you’re feeling like that, have a read of my &lt;a href=&quot;/writings/keeping-api-keys-and-environment-specifics-out-of-your-opentelemetry-config&quot;&gt;Keeping API keys and environment-specifics out of your OpenTelemetry config&lt;/a&gt; post.&lt;/p&gt;

&lt;h2 id=&quot;step-3-manual-instrumentation-support&quot;&gt;Step 3: Manual instrumentation support&lt;/h2&gt;

&lt;p&gt;Until now you haven’t made any changes to your application code; however, if you want to create your own spans and add extra metadata to your traces you’re going to need to add the OpenTelemetry libraries to your application.&lt;/p&gt;

&lt;p&gt;This was a source of confusion for me. It wasn’t clear how you were supposed to use the NodeSDK with the AWS Distro for OpenTelemetry Lambda Layer. The examples either demonstrated a non-Lambda Node setup (with clear application start and end hooks) or an auto-instrumented Lambda function.&lt;/p&gt;

&lt;p&gt;After a bit of trial and error what I realised is: you don’t need to initialise the OpenTelemetry SDK or configure any providers. The AWS Distro for OpenTelemetry Lambda Layer has done all the configuration and loaded the relevant libraries for you already. Instead, in your application code you can immediately start using the OpenTelemetry API.&lt;/p&gt;

&lt;p&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@opentelemetry/api&lt;/code&gt; to your application and use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;context.active()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trace.getSpan&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trace.setSpan&lt;/code&gt; functions. No other configuration needed.&lt;/p&gt;

&lt;p&gt;Scatter a few custom traces around your functions and redeploy. You should now start seeing these in Honeycomb too. Done.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In hindsight there’s not a lot involved in setting all this up, and it’s quite impressive once I got over a few little hurdles.&lt;/p&gt;

&lt;p&gt;The biggest challenge was that there’s a &lt;em&gt;lot of documentation&lt;/em&gt;, but nothing that fit exactly what I needed. There’s documentation on AWS Distro for OpenTelemetry with Lambda, manual instrumentation of Node.js with OpenTelemetry, auto-instrumentation of Node.js lambdas etc… and it all overlaps, but nothing gave the full picture.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Oct 2021 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/honeycomb-and-opentelemetry-with-aws-lambda-and-nodejs-reference</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/honeycomb-and-opentelemetry-with-aws-lambda-and-nodejs-reference</guid>
        
        
      </item>
    
      <item>
        <title>Getting Honeycomb working with my AWS Lambda functions</title>
        <description>&lt;p&gt;I have several existing Lambda functions which are all built on Node.js which I wanted to connect to &lt;a href=&quot;https://www.honeycomb.io/&quot;&gt;Honeycomb&lt;/a&gt;. I spent some time over the weekend working through it, and this is my stream-of-consciousness.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;If you’d like to read just the solution you can jump to &lt;a href=&quot;/writings/honeycomb-and-opentelemetry-with-aws-lambda-and-nodejs-reference&quot;&gt;Honeycomb and OpenTelemetry with Lambda and Node.js (reference)&lt;/a&gt;, and if you want to keep your API keys outside of your OpenTelemetry config you can reference environment variables like I describe in &lt;a href=&quot;/writings/keeping-api-keys-and-environment-specifics-out-of-your-opentelemetry-config&quot;&gt;Keeping API keys and environment-specifics out of your OpenTelemetry config&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, where do I start connecting these Lambdas to Honeycomb?&lt;/p&gt;

&lt;h2 id=&quot;starting-with-opentelemetry&quot;&gt;Starting with OpenTelemetry&lt;/h2&gt;

&lt;p&gt;Honeycomb &lt;a href=&quot;https://docs.honeycomb.io/getting-data-in/opentelemetry/beelines-and-otel/&quot;&gt;encourage you to use OpenTelemetry&lt;/a&gt; to send data to them. It’s nice to see a vendor encourage the use of open standards over their own client libraries (which they also have if you need them).&lt;/p&gt;

&lt;p&gt;So I start look at the &lt;a href=&quot;https://opentelemetry.io/docs/js/getting_started/nodejs/&quot;&gt;OpenTelemetry documentation for Node.js&lt;/a&gt; and it’s apparent that they are stateful application oriented, the usual set of Express web servers with their easy opportunities to run code before the server launches. Anyone who’s worked with Lambda for a while has a natural spidey-sense when you see things like this. Is this going to work in Lambda?&lt;/p&gt;

&lt;p&gt;This is my first point of confusion. I pause here for a bit and reach out to a couple of people on Twitter, and &lt;a href=&quot;https://twitter.com/lizthegrey&quot;&gt;Liz Fong-Jones&lt;/a&gt; &lt;a href=&quot;https://twitter.com/lizthegrey/status/1443588721018748936&quot;&gt;points me&lt;/a&gt; at the &lt;a href=&quot;https://aws.amazon.com/blogs/opensource/aws-distro-for-opentelemetry-adds-lambda-layers-for-more-languages-and-collector/&quot;&gt;AWS Distro for OpenTelemetry on Lambda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My confusion about Node.js support will return later, but for now Liz’s suggestion sends me off in a positive direction.&lt;/p&gt;

&lt;h2 id=&quot;aws-distro-for-opentelemetry&quot;&gt;AWS Distro for OpenTelemetry&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/otel&quot;&gt;AWS Distro for OpenTelemetry&lt;/a&gt; (aka ADOT, just rolls off the tongue) provides several pre-built &lt;a href=&quot;https://aws-otel.github.io/docs/getting-started/lambda/lambda-js&quot;&gt;Lambda Layers&lt;/a&gt; which you can add to your Lambda functions to configure OpenTelemetry for you. There’s a Node.js one, so that’s positive.&lt;/p&gt;

&lt;p&gt;Naturally, the Lambda Layer is pre-configured to export traces to X-Ray by default (which I already have in place, so not particularly helpful) but I figure it’s still valuable to see OpenTelemetry working first.&lt;/p&gt;

&lt;p&gt;I add the Lambda Layer and set the environment variables and… 💥bang💥, it falls over.&lt;/p&gt;

&lt;p&gt;I seem to have hit a &lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/issues/99&quot;&gt;known issue&lt;/a&gt; where one of the underlying OpenTelemetry JavaScript libraries seems to do something clever and try to find your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;package.json&lt;/code&gt; file which I’m not including in my bundle. I update my bundler to include the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;package.json&lt;/code&gt; in my Lambdas and… partial success?&lt;/p&gt;

&lt;p&gt;Most of my Lambdas are working, but there’s a handful which I’ve deployed with the &lt;a href=&quot;https://docs.aws.amazon.com/cdk/latest/guide/home.html&quot;&gt;AWS CDK&lt;/a&gt; that are still failing. It turns out I’ve hit &lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/issues/99&quot;&gt;a different issue&lt;/a&gt;, something about how CDK uses esbuild to package the Lambdas prevents the Lambda Layer from being able to do some meta-programming to rewire the Lambda’s handler function. I’m not really sure why this is a problem because all my other functions are also bundled with esbuild. Anyway, there’s a workaround in the Github issue and away I go. One more redeploy and…&lt;/p&gt;

&lt;p&gt;If I look in the Lambda’s logs, there’s signs of an OpenTelemetry Collector running and it’s printing about receiving traces. The traces are still going to X-Ray but it’s a step in the right direction.&lt;/p&gt;

&lt;h2 id=&quot;connecting-to-honeycomb&quot;&gt;Connecting to Honeycomb&lt;/h2&gt;

&lt;p&gt;Now to actually get the traces to Honeycomb. The AWS Distro for OpenTelemetry comes pre-packaged with a config file for the Collector which points to X-Ray. If you want your telemetry to go anywhere else you need to provide your own config file.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’m not 100% happy with the config file approach, which I’ll come back to later.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I place an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;otpl.yaml&lt;/code&gt; in the root of each of my Lambda functions which looks something like this:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;receivers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;otlp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocols&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;grpc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;exporters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;otlp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;api.honeycomb.io:443&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-honeycomb-team&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-api-key&quot;&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-honeycomb-dataset&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;environment-name&quot;&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pipelines&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;traces&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;receivers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;otlp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;exporters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;otlp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…and I set the appropriate environment variable to let the Lambda Layer know I’m providing my own config and redeploy. I invoke my function and a few seconds later data starts appearing in Honeycomb! 🎉&lt;/p&gt;

&lt;p&gt;That wasn’t so hard. Ok it wasn’t easy either, but you don’t use Node.js without expecting every new thing you try to fail with some half-baked library &lt;em&gt;laughcry&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;At this point I have auto-instrumented code sending traces to Honeycomb. The final step is to get manual instrumentation working.&lt;/p&gt;

&lt;h2 id=&quot;manual-instrumentation-with-opentelemetry&quot;&gt;Manual instrumentation with OpenTelemetry&lt;/h2&gt;

&lt;p&gt;This is the bit that I was stuck on for the longest, I think.&lt;/p&gt;

&lt;p&gt;So far, all of my code is auto-instrumented and there’s no sign of OpenTelemetry in my application code. Whilst this is a great start, I also want to be able to manually instrument certain aspects of my code-bases.&lt;/p&gt;

&lt;p&gt;The examples weren’t very helpful here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-js/tree/main/examples/basic-tracer-node&quot;&gt;simple manual setups&lt;/a&gt; rely on being able to execute code at application start time&lt;/li&gt;
  &lt;li&gt;Similarly the more complex &lt;a href=&quot;https://opentelemetry.io/docs/js/getting_started/nodejs/#setup&quot;&gt;application servers&lt;/a&gt; examples do the same&lt;/li&gt;
  &lt;li&gt;Any Lambda examples I could find relied on &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-lambda/blob/2a9c393f1fa0cc873bbe8dc8d5aa32d9eb46c158/nodejs/sample-apps/aws-sdk/src/index.ts&quot;&gt;using auto-instrumentation&lt;/a&gt; and had no use of the OpenTelemetry SDK.&lt;/li&gt;
  &lt;li&gt;Similarly, Honecomb’s examples are of &lt;a href=&quot;https://docs.honeycomb.io/getting-data-in/javascript/opentelemetry/#initialization&quot;&gt;the stateful variety&lt;/a&gt; for Node.js&lt;/li&gt;
  &lt;li&gt;And Honeycomb’s docs on AWS Lambda are &lt;a href=&quot;https://docs.honeycomb.io/getting-data-in/integrations/aws/aws-lambda/&quot;&gt;about their own Lambda Layer&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I went around in circles for a bit here. Do I need to initialise the NodeSDK like the examples are doing? If I do, how am I supposed to do that in a Lambda function (assuming I don’t want to initialise on every invocation)?&lt;/p&gt;

&lt;p&gt;Eventually, I thought I’d dig through how the Lambda Layer actually works to see if that reveals anything.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Starting at the AWS Distro for OpenTelemetry entrypoint: &lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/blob/main/nodejs/scripts/otel-handler&quot;&gt;otel-handler&lt;/a&gt;. This script is invoked instead of your Lambda’s entrypoint. It doesn’t do much except pass-on to the base OpenTelemetry Layer.&lt;/li&gt;
  &lt;li&gt;In the base OpenTelemetry Layer there’s an entrypoint too: &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-lambda/blob/2a9c393f1fa0cc873bbe8dc8d5aa32d9eb46c158/nodejs/packages/layer/scripts/otel-handler&quot;&gt;otel-handler&lt;/a&gt; and this one looks a little more interesting, it requires an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/wrapper.js&lt;/code&gt; before it invokes your own Lambda handler. So this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wrapper.js&lt;/code&gt; will be the first thing to execute (which is a common requirement of setting instrumentation, so this is getting interesting)&lt;/li&gt;
  &lt;li&gt;If we dig into the &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-lambda/blob/2a9c393f1fa0cc873bbe8dc8d5aa32d9eb46c158/nodejs/packages/layer/src/wrapper.ts&quot;&gt;wrapper.js&lt;/a&gt;… we’ve struck gold. A whole lot of calls to the OpenTelemetry JavaScript SDK.  Setting up a provider. Configuring the tracer etc…&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once I’d seen that wrapper script I thought, perhaps if OpenTelemetry has already been initialised all I need to do is just start using the OpenTelemetry API in my code and it’ll all just magically work? And I was correct! I can grab the active context with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opentelemetry.context.active()&lt;/code&gt; and start adding spans to it. No further configuration needed.&lt;/p&gt;

&lt;p&gt;My Lambda functions now just need the Lambda Layer attached and configured, and then my instrumentation code calls the OpenTelemetry API and the rest is as you’d expect. Now I have auto-instrumented code and manual instrumented code, all going via OpenTelemetry and being pushed to Honeycomb. Nice 👍&lt;/p&gt;

&lt;p&gt;But there is one last thing on my mind. The config file that AWS Distro needs you to create. That config file is where you put your API Keys and other exporter settings. That’s not ideal.&lt;/p&gt;

&lt;h2 id=&quot;making-the-opentelemetry-config-free-of-api-keys-and-environment-specifics&quot;&gt;Making the OpenTelemetry config free of API keys and environment-specifics&lt;/h2&gt;

&lt;p&gt;To add some extra context, it’s important to understand that I bundle my Lambda functions &lt;em&gt;once&lt;/em&gt; and only once. I don’t build environment-specific bundles, instead the one bundle is “promoted” to different environments. Where this becomes an issue is OpenTelemetry needs me to embed the Honeycomb settings in the config file.&lt;/p&gt;

&lt;p&gt;The first problem is the Honeycomb Dataset setting which controls how your data is bucketed in Honeycomb, and ideally should be unique for each environment. This Dataset setting has to be defined as a header on the Exporter config.&lt;/p&gt;

&lt;p&gt;I initially resolved this by creating multiple config files, one per environment, and bundling them all into the Lambda environment-agnostic package. I could then choose which file to use by setting a different &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OPENTELEMETRY_COLLECTOR_CONFIG_FILE&lt;/code&gt; environment variable value per-environment. This approach wasn’t viable long-term as I have ephemeral environments and can’t really create a config file for each environment ahead of time.&lt;/p&gt;

&lt;p&gt;The second problem is the API Key itself. I don’t want to be embedding that in a config file and committing it to my Git repo. I could generate the config file dynamically at build time and inject the API key only in the build step, but that sounded like hard work (especially if I had one file per environment).&lt;/p&gt;

&lt;p&gt;I thought there must be a way to customise the config file at runtime somehow, but the examples are &lt;em&gt;sparse&lt;/em&gt; for anything beyond the most basic use of the AWS Distro for OpenTelemetry.&lt;/p&gt;

&lt;p&gt;After a bit of searching I came across a reference to an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OTEL_EXPORTER_OTLP_ENDPOINT&lt;/code&gt; environment variable in an &lt;a href=&quot;https://github.com/aws-observability/aws-otel-collector/issues/646&quot;&gt;open Github issue&lt;/a&gt; which sent me off down a rabbit hole. If there’s one environment variable, there must be more, right? Turns out there are, there’s &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/exporter.md&quot;&gt;quite a lot of them&lt;/a&gt;. One environment variable which looked particularly promising was &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OTEL_EXPORTER_OTLP_HEADERS&lt;/code&gt; which you can set to provide a list of key-value pairs which are used as the HTTP Headers in any Exporter requests; this sounds perfect as those headers are where you set the Honeycomb Team API Key and the Dataset name. Perfect!&lt;/p&gt;

&lt;p&gt;Unfortunately, I could not get any of those environment variables to work with the AWS Distro for OpenTelemetry Lambda Layer. No combination of them seemed to make any difference. As far as I can tell, these environment variables are ignored in the Lambda Layer.&lt;/p&gt;

&lt;p&gt;The idea of environment variables stuck in my head though, and I wondered if perhaps the config parser had handling for referencing other environment variables. I found my way to the &lt;a href=&quot;https://opentelemetry.io/docs/collector/configuration&quot;&gt;OpenTelemetry Collector documentation&lt;/a&gt; which has a section on &lt;a href=&quot;https://opentelemetry.io/docs/collector/configuration/#configuration-environment-variables&quot;&gt;environment variables&lt;/a&gt; and contained these magic words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The use and expansion of environment variables is supported in the Collector configuration.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is sounding promising. I updated my Exporter config to reference some custom environment variables which I can set per Lambda without changing the bundled config file:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;exporters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;otlp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;api.honeycomb.io:443&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-honeycomb-team&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$HONEYCOMB_TEAM_KEY&quot;&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-honeycomb-dataset&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$HONEYCOMB_DATASET$&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…and it works! Success.&lt;/p&gt;

&lt;p&gt;I can now revert to a single config file which has no secrets or environment-specific information in. Each lambda then has a couple of additional environment variables which set the API key and the Dataset.&lt;/p&gt;

&lt;p&gt;I’m not a big fan of storing secrets in environment variables, but until there’s support for &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/issues/2469&quot;&gt;external secret management in OpenTelemetry&lt;/a&gt; I think this is the best I’m going to get.&lt;/p&gt;

&lt;p&gt;That’s it. All done. I can rest now.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To summarise:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use the &lt;a href=&quot;https://aws-otel.github.io/docs/getting-started/lambda/lambda-js&quot;&gt;AWS Distro for OpenTelemetry Lambda Layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Resolve any awkward issues with the newness of Node.js support (&lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/issues/99&quot;&gt;bundle your package.json&lt;/a&gt; and &lt;a href=&quot;https://github.com/aws-observability/aws-otel-lambda/issues/99&quot;&gt;resolve any CDK issues&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Use the OpenTelemetry API library directly, no need to initialise it yourself as the layer does it for you. Just grab the tracer and start creating spans.&lt;/li&gt;
  &lt;li&gt;Create a custom config file pointing to Honeycomb, and use environment variable expansion to &lt;a href=&quot;/writings/keeping-api-keys-and-environment-specifics-out-of-your-opentelemetry-config&quot;&gt;keep the config file environment-agnostic and API keys&lt;/a&gt; out of your version control.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 03 Oct 2021 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/getting-honeycomb-working-with-my-aws-lambda-functions</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/getting-honeycomb-working-with-my-aws-lambda-functions</guid>
        
        
      </item>
    
      <item>
        <title>The anatomy of a quick bash script (bulk rename Kinesis Firehose files in S3)</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Also known as&lt;/strong&gt;: how to move hundreds of files in S3 that you accidentally put in the wrong place because you misconfigured a Kinesis Firehose.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;p&gt;I have a Kinesis Firehose streaming change capture data into an S3 bucket so I can query it using Athena. This has been working great for months, but I just realised the default configuration of Firehose does not put the files in a structure that Athena can partition on, so my Athena queries are scanning the entire dataset instead of partitioning on date.&lt;/p&gt;

&lt;p&gt;Kinesis Firehose by default delivers files in a structure like: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2020/07/20/16/Filename-2020-07-20-16-00-00-hash&lt;/code&gt;. But Athena wants files in a structure compatible with Hive’s partition format, which uses a variable-based convention: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;year=2020/month=07/day=20/hour=16/Filename-2020-07-20-16-00-00-hash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In my case, I’ve decided having the hour in the path is making my partitions too small (not enough data per-hour) so I’m going to use a structure of just the date as a single variable: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dt=2020-07-20/Filename-2020-07-20-16-00-00-hash&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now I just need to move hundreds of files from the old structure into the new structure!&lt;/p&gt;

&lt;h2 id=&quot;the-commands&quot;&gt;The commands&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recursive&lt;/span&gt; s3://bucket-name/ &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;{ print $4 }&apos;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  | &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-E&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;p;s/([0-9]{4})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;([0-9]{2})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;([0-9]{2})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;[0-9]{2}/dt=&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\2&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\3&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  | &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-E&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;s/^/s3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;bucket-name&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  | xargs &lt;span class=&quot;nt&quot;&gt;-n2&lt;/span&gt; aws s3 &lt;span class=&quot;nb&quot;&gt;mv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this script will move all files from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s3://bucket-name/year/month/day/hour/filename&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s3://bucket-name/dt=year-month-day/filename&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: this is not particularly elegant: you could likely combine several of these lines together but I’ve favoured readability over efficiency, and it’s also not particularly fast so probably don’t use it for millions of files.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-breakdown&quot;&gt;The breakdown&lt;/h2&gt;

&lt;p&gt;Start by listing all the files in the bucket:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 ls --recursive s3://bucket-name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This produces an output like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2020-07-16 15:12:29 10016 2020/07/16/05/Snapshot-2020-07-16-05-07-26
2020-07-17 09:21:12 10838 2020/07/16/23/Snapshot-2020-07-16-23-16-10
2020-07-17 09:27:52  5790 2020/07/16/23/Snapshot-2020-07-16-23-22-50
2020-07-17 09:55:43  5409 2020/07/16/23/Snapshot-2020-07-16-23-50-41
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’m truncating the filenames in this post just to make it a bit more readable. Imagine there being
a hash at the end of each line too.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next, extract just the filename from the output, which is the 4th column:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;{ print $4 }&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’m using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awk&lt;/code&gt; here because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; doesn’t deal well with space delimited columns and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 ls&lt;/code&gt; doesn’t let you control the output format. If the output was tab delimited &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; would’ve worked too.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This produces an output of just the filenames:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2020/07/16/05/Snapshot-2020-07-16-05-07-26
2020/07/16/23/Snapshot-2020-07-16-23-16-10
2020/07/16/23/Snapshot-2020-07-16-23-22-50
2020/07/16/23/Snapshot-2020-07-16-23-50-41
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next is the fun part. We need to take that filename and transform it into our new format &lt;em&gt;whilst also preserving the original filename so both can be passed to a move command together&lt;/em&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sed&lt;/code&gt; works well for this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-E&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;p;s/([0-9]{4})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;([0-9]{2})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;([0-9]{2})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;[0-9]{2}/dt=&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\2&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\3&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s a pretty dense command, but most of that is a regular expression. To break it down:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-E&lt;/code&gt; enable extended regular expressions, this gives us the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;()&lt;/code&gt; group match support.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p;&lt;/code&gt; you might not see this part very often, this prints the input as well as the replacement, so for one line in you get two lines out: the original, and whatever the replacement produced. This is key for being able to build the move command easily later as it requires a source and a destination.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s/&lt;/code&gt; start the regular expression.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;([0-9]{4})\/([0-9]{2})\/([0-9]{2})\/[0-9]{2}&lt;/code&gt; match the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;year/month/day/hour&lt;/code&gt; part of the path, capturing the year, month, and day as groups so they can be reused later.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dt=\1-\2-\3&lt;/code&gt; specify the replacement; we replace the whole match (year/month/day/hour) with a pattern of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dt=year-month-day&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\1&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\2&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\3&lt;/code&gt; are references to the capture groups in the regular expression.&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;Optimisation: This could probably have done in our previous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awk&lt;/code&gt; command which has regular expression support, but it was looking pretty ugly and I’d prefer to waste a few CPU cycles than bend my brain too much.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The output now looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2020/07/16/05/Snapshot-2020-07-16-05-07-26
dt=2020-07-16/Snapshot-2020-07-16-05-07-26
2020/07/16/23/Snapshot-2020-07-16-23-16-10
dt=2020-07-16/Snapshot-2020-07-16-23-16-10
2020/07/16/23/Snapshot-2020-07-16-23-22-50
dt=2020-07-16/Snapshot-2020-07-16-23-22-50
2020/07/16/23/Snapshot-2020-07-16-23-50-41
dt=2020-07-16/Snapshot-2020-07-16-23-50-41
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You’ll notice there are twice as many lines now, due to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p;&lt;/code&gt; setting of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sed&lt;/code&gt;. For each original line there’s now that line plus the reformatted line.&lt;/p&gt;

&lt;p&gt;Next up is to prefix each line with the bucket name. The move command expects all filenames to be fully-qualified with their bucket:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-E&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;s/^/s3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;bucket-name&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Optimisation: With some smarter regular expressions, this could’ve been folded into the previous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sed&lt;/code&gt; statement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The output now looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;s3://bucket-name/2020/07/16/05/Snapshot-2020-07-16-05-07-26
s3://bucket-name/dt=2020-07-16/Snapshot-2020-07-16-05-07-26
s3://bucket-name/2020/07/16/23/Snapshot-2020-07-16-23-16-10
s3://bucket-name/dt=2020-07-16/Snapshot-2020-07-16-23-16-10
s3://bucket-name/2020/07/16/23/Snapshot-2020-07-16-23-22-50
s3://bucket-name/dt=2020-07-16/Snapshot-2020-07-16-23-22-50
s3://bucket-name/2020/07/16/23/Snapshot-2020-07-16-23-50-41
s3://bucket-name/dt=2020-07-16/Snapshot-2020-07-16-23-50-41
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The same as before, but now prefixed with the bucket name.&lt;/p&gt;

&lt;p&gt;One final step to go, the actual move:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xargs &lt;span class=&quot;nt&quot;&gt;-n2&lt;/span&gt; aws s3 &lt;span class=&quot;nb&quot;&gt;mv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use xargs (an absolute cornerstone of shell scripting) to execute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 mv&lt;/code&gt; command and finish the job. Let’s break this one down too:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xargs&lt;/code&gt; reads from stdin and executes a command with the stdin lines as arguments. So &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo hi | xargs cowsay&lt;/code&gt; is the equivalent to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cowsay hi&lt;/code&gt;. In our case, we take the filenames we’ve been processing and pass them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 mv&lt;/code&gt; as arguments.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-n2&lt;/code&gt; this tells xargs to take &lt;em&gt;two lines&lt;/em&gt; instead of just one, and pass them both to our command. Because we have one line with the original and a second line with the transformed, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xargs&lt;/code&gt; calls the move command like so: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 mv original-line transformed-line&lt;/code&gt;, which is exactly what’s need to move a file from one place to another.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 mv&lt;/code&gt; as already mentioned, is the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xargs&lt;/code&gt; is going to call.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Running that command will take a list of original filenames, transform them into pairs of original and transformed filenames, prepend the filenames with the bucket, and then pass the pairs to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 mv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And that’s it, done! Files moved.&lt;/p&gt;

</description>
        <pubDate>Tue, 21 Jul 2020 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/bulk-s3-move</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/bulk-s3-move</guid>
        
        
      </item>
    
      <item>
        <title>InfluxDB Kapacitor subscription errors</title>
        <description>&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Post http://kapacitor.default:9092/write?consistency=&amp;amp;db=telegraf&amp;amp;precision=ns&amp;amp;rp=autogen: dial tcp: lookup kapacitor.default on 100.1.1.1:53: no such host service=subscriber
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you’re seeing something like this error in your &lt;a href=&quot;https://github.com/influxdata/influxdb&quot;&gt;InfluxDB&lt;/a&gt; logs, and don’t know what it means: &lt;a href=&quot;https://github.com/influxdata/kapacitor&quot;&gt;Kapacitor&lt;/a&gt; has created one or more subscriptions in your InfluxDB database, and InfluxDB is trying to POST to the Kapacitor endpoint; however, Kapacitor is unreachable. Kapacitor might be unreachable because it’s down, or you have a network partition or other connectivity issue, or in my case you’ve actually just destroyed your Kapacitor instance.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;To fix this error, you need to remove the subscriptions; you can remove subscriptions by issuing a few commands to InfluxDB via your favourite interface (for me, it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exec&lt;/code&gt; into a container and running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;influx&lt;/code&gt; cli tool, but you can also use the API).&lt;/p&gt;

&lt;p&gt;First, find the subscription(s) you need to remove.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SHOW SUBSCRIPTIONS

name: telegraf
retention_policy name          mode destinations
---------------- ----          ---- ------------
autogen          kapacitor-abc ANY  [http://kapacitor.default:9092]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then just drop the subscription (you might need to drop a few, if you have multiple databases).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DROP SUBSCRIPTION kapacitor-abc ON telegraf.autogen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The format of the command is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DROP SUBSCRIPTION &amp;lt;subscription&amp;gt; ON &amp;lt;database&amp;gt;.&amp;lt;retention_policy&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That’s it, the error should stop occurring now. If you destroy a Kapacitor instance, remember to remove it’s subscriptions until there’s resolution on &lt;a href=&quot;https://github.com/influxdata/kapacitor/issues/870&quot;&gt;subscription cleanups&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Mar 2017 00:00:00 +1100</pubDate>
        <link>https://www.jagregory.com//writings/influxdb-kapacitor-subscription-error</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/influxdb-kapacitor-subscription-error</guid>
        
        
      </item>
    
      <item>
        <title>Docker container out of disk space</title>
        <description>&lt;p&gt;Are programs in your Docker container complaining of no free space?&lt;br /&gt;
Does your host have loads of space?&lt;br /&gt;
And your container does too?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;It could be inode exhaustion!&lt;/em&gt;&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;the-symptoms&quot;&gt;The symptoms&lt;/h2&gt;

&lt;p&gt;All of a sudden my CI agent (which is in a Docker container) stopped running builds. Everything went &lt;span style=&quot;color:red&quot;&gt;red&lt;/span&gt;. The failures were inconsistent, happening at different points in the build but always failing.&lt;/p&gt;

&lt;p&gt;A typical error looked something like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm ERR! ENOSPC, open /var/lib/go-agent/pipelines/{blah}/node_modules/node-sass/node_modules/...
npm ERR! nospc This is most likely not a problem with npm itself and is related to insufficient space on your system.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ENOSPC and “Insufficient space on your system” are dead giveaways that something is wrong! So npm thinks there isn’t any space on the disk.&lt;/p&gt;

&lt;p&gt;I best look into this.&lt;/p&gt;

&lt;h2 id=&quot;diagnosing&quot;&gt;Diagnosing&lt;/h2&gt;

&lt;p&gt;I SSH onto the box and have a poke around.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ df -h

Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       99G  9.4G   85G  10% /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df&lt;/code&gt; tells me there’s heaps of space available on the host. This is unsurprising because I’ve just resized the disk, but it’s worth checking.&lt;/p&gt;

&lt;p&gt;Next I run the same command in the agent container.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker exec agent-1 df -h

Filesystem                   Size  Used Avail Use% Mounted on
/dev/mapper/docker-202:1-... 9.8G  1.8G  7.5G  20% /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Container says the same thing, of the &lt;a href=&quot;https://jpetazzo.github.io/2014/01/29/docker-device-mapper-resize/&quot;&gt;10gb Docker allocates a container by default&lt;/a&gt; there’s 7.5gb still available.&lt;/p&gt;

&lt;p&gt;Nothing is out of disk space.&lt;/p&gt;

&lt;p&gt;So what could prevent creating new files and masquerade as lack of free space? &lt;a href=&quot;https://en.wikipedia.org/wiki/Inode&quot;&gt;inodes&lt;/a&gt; can!&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker exec agent-1 df -i

Filesystem                   Inodes  IUsed   IFree IUse% Mounted on
/dev/mapper/docker-202:1-... 655360 655360       0  100% /
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df&lt;/code&gt; with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-i&lt;/code&gt; flag reports on inode usage. Oh no, 100% of inodes within my container are in use. That’s not good. Not good at all. &lt;em&gt;Problem identified&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;finding-where-all-our-inodes-are&quot;&gt;Finding where all our inodes are&lt;/h2&gt;

&lt;p&gt;An inode can be thought of as a pointer to a file or directory with a bit of data about them. All those file permissions and owners you set on files get stored in the file’s inode. Therefore, for every file or directory you have there’ll be a corresponding inode. So if we’ve used all our inodes, we’ve used all our available files. We have too many files. You can have a lot of files, so running out is a sign of &lt;em&gt;something bad happening&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;With this in mind, we have to find where all these mysterious files are and why there are so many. I read a &lt;a href=&quot;http://stackoverflow.com/questions/653096/howto-free-inode-usage&quot;&gt;free inode usage&lt;/a&gt; post on StackOverflow which has some handy commands for answering this question.&lt;/p&gt;

&lt;p&gt;I ran this command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo find . -xdev -type f | cut -d &quot;/&quot; -f 2 | sort | uniq -c | sort -n
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which prints out the inode count for all subdirectories of the current directory. It will take a little while, then print out something like this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   89 opt
  101 sbin
  109 bin
  258 lib
  651 etc
  930 root
23466 usr
83629 var
51341 tmp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;OH HELLO &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tmp&lt;/code&gt; WHY DO YOU HAVE SO MANY INODES?&lt;/p&gt;

&lt;h2 id=&quot;fixing-the-issue&quot;&gt;Fixing the issue&lt;/h2&gt;

&lt;p&gt;One &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm -r /tmp&lt;/code&gt; later and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df -i&lt;/code&gt; reports a much more healthy 20% inode usage. Easy when you know what the problem is.&lt;/p&gt;

&lt;p&gt;In my case it was a stupid lack of cleaning up some temp files from our builds. An &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;npm install&lt;/code&gt; which downloads the entire world into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp&lt;/code&gt; on every build. We were averaging 10000 inodes per build. Ouch.&lt;/p&gt;

&lt;p&gt;Builds are now &lt;span style=&quot;color:green&quot;&gt;green&lt;/span&gt; again. Pretty anti-climactic; but it’s the journey not the destination which counts, right?&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Jun 2015 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/docker-container-out-of-space</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/docker-container-out-of-space</guid>
        
        
      </item>
    
      <item>
        <title>Sinopia: a private NPM registry</title>
        <description>&lt;p&gt;Not all of our packages can be pushed to the public NPM repository. Proprietary code and uninteresting code we want to keep internal, but until recently the package distribution story for this code has been worse than open-sourcing it. You either modularise it and publish to the world, or you have a bad time.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Or you modularise and share via git dependencies, which isn’t a great solution. You lose versioning, all the pre-publish hook loveliness, and become quite limited by where our package is being used (no preprocessors for you!).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the early days NPM wasn’t designed with multiple registries in mind, so hosting your own internal one meant either proxying/mirroring the public registry or manually adding public packages to your private registry and using it for everything. Thanks to improvements in NPM and several open-source efforts, it’s now much easier than that to host your own internal NPM registry.&lt;/p&gt;

&lt;p&gt;NPM have recently &lt;a href=&quot;https://www.npmjs.com/private-modules&quot;&gt;started offering private modules&lt;/a&gt;, which looks very interesting. There’s also a pay-for &lt;a href=&quot;http://npmjs.com/enterprise#pricing&quot;&gt;Enterprise option&lt;/a&gt; from NPM which is worth thinking about once you scale.&lt;/p&gt;

&lt;p&gt;If private cloud hosting isn’t your thing, the host-your-own options are worth exploring. &lt;a href=&quot;https://www.npmjs.com/package/sinopia&quot;&gt;Sinopia&lt;/a&gt; is what we’ll setup.&lt;/p&gt;

&lt;h2 id=&quot;what-is-sinopia&quot;&gt;What is Sinopia?&lt;/h2&gt;

&lt;p&gt;Sinopia is a simple NPM registry, which has zero dependencies (that means no CouchDB for those who’ve done this before); it has support for most day-to-day NPM features, such as &lt;a href=&quot;https://docs.npmjs.com/misc/scope&quot;&gt;scopes&lt;/a&gt;, &lt;a href=&quot;https://docs.npmjs.com/cli/publish&quot;&gt;command line publishing&lt;/a&gt;, and &lt;a href=&quot;https://docs.npmjs.com/cli/adduser&quot;&gt;authentication&lt;/a&gt;. Sinopia is a good candidate for a low-to-medium utilised NPM registry, such as company private registries.&lt;/p&gt;

&lt;h2 id=&quot;running-sinopia-on-your-machine&quot;&gt;Running Sinopia on your machine&lt;/h2&gt;

&lt;p&gt;If you just want to run Sinopia locally and aren’t interested in how it works, you can:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/jagregory/sinopia-ansible.git
cd sinopia-ansible
ansible-galaxy install -r requirements.txt
vagrant up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once this completes Sinopia will be available on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4873&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;vagrant-setup&quot;&gt;Vagrant setup&lt;/h2&gt;

&lt;p&gt;Vagrant uses a Vagrantfile to define how your Virtual Machine (or machines) will be configured. You can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vagrant init&lt;/code&gt; to create a Vagrantfile. Once you have that file, we’ll make some changes inside the config block.&lt;/p&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;The Ubuntu boxes provided by the Phusion team are a good set of boxes to start from. We’re using their 14.04 TLS box.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;box&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;ubuntu-14.04-amd64-vbox&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;box_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;https://oss-binaries.phusionpassenger.com/vagrant/boxes/latest/ubuntu-14.04-amd64-vbox.box&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;When Sinopia starts it will listen on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4873&lt;/code&gt; in the guest machine, but we need to forward that port to a port on our host. For convinience, we’ll just use the same port on both.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;forwarded_port&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;guest: &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4873&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;host: &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4873&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;Finally, Vagrant will run Ansible on the Virtual Machine. Ansible will download and configure Sinopia using the &lt;a href=&quot;https://github.com/jagregory/sinopia-ansible&quot;&gt;sinopia-ansible&lt;/a&gt; role.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;provision&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;ansible&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ansible&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ansible&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;playbook&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;site.yml&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;h2 id=&quot;ansible-playbook&quot;&gt;Ansible playbook&lt;/h2&gt;

&lt;p&gt;Our Vagrantfile delegates machine setup to Ansible, which is driven by a &lt;a href=&quot;http://docs.ansible.com/playbooks.html&quot;&gt;Playbook&lt;/a&gt;. Our playbook is pretty simple, but it’s worth having a look.&lt;/p&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;Here’s the directory structure we have for Ansible. There’s very little to it.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── Vagrantfile
├── hosts
├── requirements.txt
└── site.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;&lt;strong&gt;site.yml&lt;/strong&gt;: For our Vagrantfile to run we need a site.yml Ansible playbook, which tells Ansible which roles our host is supposed to have.&lt;/p&gt;

    &lt;p&gt;In our site.yml we give the playbook a name, specify that it should run on all of our hosts (we only have one), that our commands should be run as sudo, and then we specify which roles our machine should have.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
- name: Install Sinopia
  hosts: all
  sudo: True
  roles:
    - nodesource.node
    - jagregory.sinopia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;p&gt;&lt;strong&gt;requirements.txt&lt;/strong&gt;: Ansible has a dependency management system/tool called &lt;a href=&quot;https://galaxy.ansible.com/&quot;&gt;Ansible Galaxy&lt;/a&gt; which is growing in popularity for sharing roles. We’ll use Ansible Galaxy to download the roles we use for this machine, rather than copying them into our repository.&lt;/p&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;Ansible Galaxy uses a requirements.txt to list which dependencies to install. We just have two, a NodeJS role and our Sinopia role.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nodesource.node
jagregory.sinopia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;p&gt;To install these dependencies you can run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible-galaxy install -r requirements.txt&lt;/code&gt;&lt;/p&gt;

&lt;section class=&quot;side-by-side&quot;&gt;
  &lt;div&gt;

    &lt;p&gt;&lt;strong&gt;hosts&lt;/strong&gt;: The last thing of interest is the Ansible inventory file, which declares the machines we’re letting Ansible manage. In our case it’s just one host in our inventory.&lt;/p&gt;

  &lt;/div&gt;

  &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm.jagregory.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/section&gt;

&lt;h2 id=&quot;sinopia-in-the-cloud&quot;&gt;Sinopia in the Cloud&lt;/h2&gt;

&lt;p&gt;If you want to run a Sinopia instance in the cloud it’s as easy as launching an instance in EC2 (or your preferred provider), adding it’s public IP address to the inventory file and running Ansible.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ansible-playbook site.yml -i hosts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will run Ansible against all the hosts in our inventory (hosts file) and execute the site.yml playbook.&lt;/p&gt;

&lt;p&gt;You can read more about running Sinopia &lt;a href=&quot;https://www.npmjs.com/package/sinopia&quot;&gt;on their website&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 May 2015 00:00:00 +1000</pubDate>
        <link>https://www.jagregory.com//writings/sinopia-private-npm-registry</link>
        <guid isPermaLink="true">https://www.jagregory.com//writings/sinopia-private-npm-registry</guid>
        
        
      </item>
    
  </channel>
</rss>
